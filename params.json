{
  "name": "Tcga-cluster-scripts",
  "tagline": "A short guide to downloading and manipulating TCGA data on a TCGA cluster",
  "body": "# Genomic Data Commons\r\n[Introduction to the GDC Portal](https://gdc.nci.nih.gov/access-data/gdc-data-portal)\r\n\r\n[GDC Data Portal for TCGA Data](https://gdc-portal.nci.nih.gov/search/s)\r\n\r\n[Guide to Downloading TCGA Data using the New GDC](https://gdc-docs.nci.nih.gov/Data_Transfer_Tool/Users_Guide/)\r\n\r\n[Which GDC tools to use to download TCGA data](https://gdc.nci.nih.gov/access-data/data-access-processes-and-tools)\r\n\r\n***\r\n\r\n# Introduction to the ATRF Cluster\r\nThe NCI-ATRF cluster, which is a Moab HPC cluster, is a system of connected nodes, each representing a computer. When you first connect to the cluster *(see Sample Workflow: Connecting to the Cluster)*, you will be connected to the head node, which you can use for basic commands (ls, cd, mkdir), but you should not use it for any extensive scripts (otherwise it will slow down everyone else at NCI connected to the server on the head node).\r\n\r\nBecause of this, you should request a node to use in interactive mode `qsub -I` as soon as you connect, which essentially is allowing you to use a computer in the cluster that no one else is using. You can run small programs in this mode without doing anything else (./my-small-program), but this prevents you from being able to turn off your computer or closing the terminal without stopping whatever programs you ran in this node.\r\n\r\nTo allow programs to run 24/7 on the server regardless of whether your computer is on, you will need to submit a ***job*** to the server, which you can do with or without having started an interactive `qsub -I` node. To submit a job, you will first need to create a ***job file*** (call it myjobfile.pbs) in this format:\r\n\r\n```bash\r\n#!/bin/bash\r\n#PBS -l nodes=1:ppn=1\r\n\r\ncd $PBS_O_WORKDIR\r\necho 'Hello World' > output.txt\r\n```\r\nThe first line is just a header to tell the interpreter to run bash.\r\n\r\nThe second line starting with #PBS, is an option saying to run the job with 1 node and 1 processor. PBS options allow you to specify how you want the job to run (maximum time before it closes, how many processors to use, whether to email you if the job has finished, etc).\r\n\r\nThe third line makes use of the variable ```$PBS_O_WORKDIR```, which will evaluate to the directory in which you ran the script. If you do not ```cd $PBS_O_WORKDIR``` at the beginning of your job, the job will run at the ~ root folder instead.\r\n\r\nTo submit the job file (*myjobfile.pbs*), enter in the command:\r\n```bash\r\nqsub myjobfile.pbs\r\n```\r\nThe cluster will then begin running the commands specified in the job file. The Hello World job above will run and finish extremely quickly, but if you are running a file that takes a longer amount of time, you will probably want to check the status of your jobs at some point. To check the status of your running jobs, you can enter in the command:\r\n```bash\r\npbsn -u username\r\n```\r\nHere is a real-world example of a job file that runs a bam-indexing script (not pictured here) and forwards the error and logs to specified directories:\r\n\r\n```bash\r\n#!/bin/bash\r\n#PBS -N bam-index\r\n#PBS -q medium   \r\n#PBS -l nodes=1\r\n#PBS -l cput=24:00:00        \r\n#PBS -m bea\r\n#PBS -M chenk8@mail.nih.gov\r\n\r\ncd $PBS_O_WORKDIR\r\n./bam-index.sh > ~/err/$PBS_JOBID > ~/logs/$PBS_JOBID\r\nexit 0\r\n```\r\n\r\n[Further explanation on the various job script options can be found here.](https://hpcc.usc.edu/support/documentation/running-a-job-on-the-hpcc-cluster-using-pbs/)\r\n\r\n***\r\n\r\n# Sample workflow\r\n## Login to the Moab HPC cluster\r\nOpen your terminal and log in with your NIH username and password:\r\n```bash\r\nssh USERNAME@moab.ncifcrf.gov\r\n```\r\n\r\nThen, open up an interactive job (so that the commands/scripts that you run\r\nwon't slow down the head node of the server):\r\n```bash\r\nqsub -I\r\n```\r\nEnter in `exit` if you ever want\r\nto leave it.\r\n\r\nChange to the lab directory:\r\n```bash\r\ncd /ifs/projects/GRCBL-NGS/slowtemp\r\n```\r\n\r\n## Start a project\r\nCreate a project directory:\r\n```bash\r\nmkdir project1/\r\ncd project1/\r\n```\r\n\r\nCreate the cghub key:\r\n```bash\r\nnano cghub.key\r\n```\r\nThen copy and paste your TCGA key (in your email) into the folder, and enter Ctrl+X and then Y\r\nto save.\r\n\r\nDownload basic TCGA and BAM scripts and set them as executable:\r\n```bash\r\ngit clone https://github.com/kevchn/tcga-cluster-scripts scripts/\r\nchmod +x scripts/*.sh\r\n```\r\n\r\n## Getting your TCGA authentication token\r\nYou will need an authentication token to download controlled data from TCGA (most raw-sequencing files). Minimize the terminal (but do not close it).\r\n\r\nTo generate a token, first log in to the [GDC Data Portal](https://gdc-portal.nci.nih.gov/) by clicking the Login button in the top right corner of the page.\r\n\r\nAfter logging in, clicking the username will open a drop-down menu. Select Download Token from the menu to generate an authentication token. Copy the contents of this token and save it `nano token.txt` in the project1/ directory.\r\n\r\n## Finding the UUID of a single TCGA sample\r\nHead to the [Genomic Data Commons Data Portal](https://gdc-portal.nci.nih.gov/search/s)\r\n\r\nClick on any specific file to get more information, including its UUID (for example: 22a29915-6712-4f7a-8dba-985ae9a1f005)\r\n\r\n## Downloading a single TCGA file onto the server\r\nIf you have the UUID of a single TCGA file, use the new [gdc-data-transfer-tool](https://gdc.nci.nih.gov/access-data/gdc-data-transfer-tool):\r\n```bash\r\nmodule add gdc-client\r\ngdc-client download 22a29915-6712-4f7a-8dba-985ae9a1f005 -t token.txt\r\n```\r\nPlease note that the NCI-ATRF cluster may not have the gdc-client installed, or it may be under a different name, in which case the above commands will not work. Enter in `module list` to see if there are any modules called gdc, genomic-data-commons, or similar, and import that module instead. If no modules exist, contact the NCI helpdesk to request that the program be installed.\r\n\r\n[You can also download the gdc-client locally on your computer](https://gdc.nci.nih.gov/access-data/gdc-data-transfer-tool).\r\n\r\n## Download a list of desired TCGA files\r\nMinimize the terminal (but do not close it), and head to the [Genomic Data Commons Data Portal](https://gdc-portal.nci.nih.gov/search/s)\r\n\r\nUse the sidebar to filter out the desired samples. For example, if you want all\r\nGBM RNA-seq samples, under the Cases tab, select Primary Site>Brain and Project>TCGA-GBM, and then under the Files tab, select Data Category>Raw Sequencing Data and Experimental Strategy>RNA-seq. The end-result should be 174 files.\r\n\r\nClick the Download Manifest button to download an xml file containing the IDs of all these files (so that you can download them onto the cluster). [More information regarding how to download individual and multiple IDs from TCGA can be found here.](https://gdc-docs.nci.nih.gov/Data_Transfer_Tool/Users_Guide/Preparing_for_Data_Download_and_Upload/)\r\n\r\nOpen the xml file, copy the contents, and then open up terminal again.\r\n\r\nIn your project1/ folder, `nano manifest.xml` and paste in the XML information before saving (Ctrl+X, then Y).\r\n\r\n## Bulk downloading TCGA files onto the server\r\nOnce you have the manifest.xml file, you can download the files using the new [gdc-data-transfer-tool](https://gdc.nci.nih.gov/access-data/gdc-data-transfer-tool):\r\n```bash\r\nmodule add gdc-client\r\ngdc-client download -m manifest.xml -t token.txt\r\n```\r\n\r\nYou will most likely want to do this using a job script, so that the cluster will download the files 24/7 until the files are all downloaded.\r\n***\r\n# Running this repository's jobfiles and scripts\r\nThe following usage commands assume that you are in the parent directory of the scripts/ folder. If you are in a different directory, for example, projects1/data, you will have to provide either a different relative path for usage: ```qsub ../scripts/index-bams.pbs``` or an absolute path: ```qsub /ifs/projects/GRCBL-NGS/slowtemp/project1/scripts/index-bams.pbs```.\r\n\r\n## scripts/index-bams.pbs\r\nIndexes (and possibly sorts) all .bam files in the current directory\r\n\r\nCreates .bai files for all .bam files in the current directory. Doesn't change the original .bam files. Need to ensure that the bam files are already sorted. TCGA files are usually sorted. To turn on sorting before indexing, ```nano scripts/index-bams.pbs``` and remove the '#' from line 15 and 16 and comment out line 17 like below:\r\n```bash\r\nsamtools sort \"${bamfile}\" \"${bamfile}.sorted\"\r\nsamtools index ${bamfile}.sorted > err/$PBS_JOBID > logs/$PBS_JOBID\r\n# samtools index $bamfile > err/$PBS_JOBID > logs/$PBS_JOBID\r\n```\r\n\r\nUsage:\r\n* ```qsub scripts/index-bams.pbs```\r\n\r\nInput:\r\n* sample1.bam\r\n* sample2.bam\r\n* sample3.bam\r\n\r\nReturns:\r\n* sample1.bai\r\n* sample2.bai\r\n* sample3.bai\r\n\r\n## scripts/separate-dif-len-bams.pbs\r\nMoves .bam files to new directories based on read-length and creates a summary of .bam file read-lengths\r\n\r\nChecks the read-length of the .bam files by looking at the header of the bam files and then moves the files into a new directory based on that read-length. Removes bam-files with multiple read-lengths. Used for programs that can only run on multiple .bam files of the same read-length. Summary file, bam-information.txt, gives read-count and read-length of each bam-file.\r\n\r\nUsage:\r\n* ```qsub scripts/separate-dif-len-bams.pbs```\r\n\r\nInput:\r\n* sample1.bam\r\n* sample2.bam\r\n* sample3.bam\r\n\r\nReturns:\r\n* bam-information.txt\r\n* bams-50/sample1.bam\r\n* bams-50/sample2.bam\r\n* bams-75/sample3.bam\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}